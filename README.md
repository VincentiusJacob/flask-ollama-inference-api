# Quick Setup Guide

This guide helps you quickly set up and test the Ollama Flask API project.

### Step 1: Install Ollama

**macOS:**

```bash
brew install ollama
```

**Linux:**

```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

**Windows:**
Download installer from [https://ollama.ai/download](https://ollama.ai/download)

---

### Step 2: Download the Model

```bash
ollama pull mistral:latest
```

**Download size:** ~4.4 GB

---

### Step 3: Verify Ollama

```bash
ollama list
```

**Expected output:**

```
NAME              ID              SIZE      MODIFIED
mistral:latest    ............    4.4 GB    X minutes ago
```

---

### Step 4: Start Ollama Service

```bash
ollama serve
```

---

### Step 5: Install Python Dependencies

**Open a NEW terminal:**

```bash
# Clone the repository
git clone https://github.com/VincentiusJacob/flask-ollama-inference-api.git

# Navigate to project directory
cd flask-ollama-inference-api

# Create virtual environment
python -m venv venv

# Activate it
source venv/bin/activate  # macOS/Linux
# OR
venv\Scripts\activate     # Windows

# Install dependencies
pip install -r requirements.txt
```

---

### Step 6: Run the Flask App

```bash
python app.py
```

---

### Step 7: Test the API

## Testing with Terminal

```bash
curl -X POST http://localhost:6000/inference \
  -H "Content-Type: application/json" \
  -d '{"prompt": "When did Lionel Messi won his first Ballon Dor?"}'
```

**Expected response:**

```json
{
  "response": "...(response generated by the model)"
}
```

---

## ðŸ§ª Quick Test Cases

### Test 1: Basic Inference

```bash
curl -X POST http://localhost:6000/inference \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Explain Flask in one sentence"}'
```

### Test 2: Error Handling - Missing Prompt

```bash
curl -X POST http://localhost:6000/inference \
  -H "Content-Type: application/json" \
  -d '{}'
```

**Expected:** `400 Bad Request` with error message

### Test 3: Error Handling - Empty Prompt

```bash
curl -X POST http://localhost:6000/inference \
  -H "Content-Type: application/json" \
  -d '{"prompt": ""}'
```

**Expected:** `400 Bad Request` with error message

---

## Testing with Postman

1. **Method:** POST
2. **URL:** `http://localhost:6000/inference`
3. **Headers:** `Content-Type: application/json`
4. **Body (raw JSON):**
   ```json
   {
     "prompt": "What is machine learning?"
   }
   ```
5. **Click Send**
