# Quick Setup Guide

This guide helps you quickly set up and test the Ollama Flask API project.

Before starting, open your terminal (Command Prompt / PowerShell on Windows, or Terminal on macOS/Linux).
All commands below will be run from a terminal window.

### Step 1: Install Ollama

**macOS:**

```bash
brew install ollama
```

**Linux:**

```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

**Windows:**
Download installer for Windows from [https://ollama.ai/download](https://ollama.ai/download)

---

### Step 2: Download the Model

In your terminal, run:

```bash
ollama pull mistral:latest
```

**Download size:** ~4.4 GB

---

### Step 3: Verify Ollama

Still in the same terminal, check if Ollama is working:

```bash
ollama list
```

**Expected output:**

```
NAME              ID              SIZE      MODIFIED
mistral:latest    ............    4.4 GB    X minutes ago
```

---

### Step 4: Start Ollama Service

Keep Ollama running while using the API
Start it in your first terminal window

```bash
ollama serve
```

Keep this terminal open, as it runs the background model server that Flask will connect to.

---

### Step 5: Setup Flask App

Open a new terminal window so Ollama keeps running in the first one

**Open a NEW terminal:**
In this new terminal:

```bash
# Clone the repository
git clone https://github.com/VincentiusJacob/flask-ollama-inference-api.git

# Navigate to project directory
cd flask-ollama-inference-api

# Create virtual environment
python -m venv venv

# Activate it
source venv/bin/activate  # macOS/Linux
# OR
venv\Scripts\activate     # Windows

# Install dependencies
pip install -r requirements.txt
```

---

### Step 6: Run the Flask App

In the same terminal (where Flask is set up), start the server:

```bash
python app.py
```

---

### Step 7: Test the API

## Testing with Terminal

```bash
curl -X POST http://localhost:6000/inference \
  -H "Content-Type: application/json" \
  -d '{"prompt": "When did Lionel Messi won his first Ballon Dor?"}'
```

**Expected response:**

```json
{
  "response": "...(response generated by the model)"
}
```

---

## ðŸ§ª Quick Test Cases

### Test 1: Basic Inference

```bash
curl -X POST http://localhost:6000/inference \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Explain Flask in one sentence"}'
```

### Test 2: Error Handling - Missing Prompt

```bash
curl -X POST http://localhost:6000/inference \
  -H "Content-Type: application/json" \
  -d '{}'
```

**Expected:** `400 Bad Request` with error message

### Test 3: Error Handling - Empty Prompt

```bash
curl -X POST http://localhost:6000/inference \
  -H "Content-Type: application/json" \
  -d '{"prompt": ""}'
```

**Expected:** `400 Bad Request` with error message

---

## Testing with Postman

1. Open **Postman**
1. Set **Method**: POST
1. Set **URL:** `http://localhost:6000/inference`
1. Under **Headers:** add `Content-Type: application/json`
1. Under **Body (raw -> JSON):** add
   ```json
   {
     "prompt": "What is machine learning?"
   }
   ```
1. **Click Send**
